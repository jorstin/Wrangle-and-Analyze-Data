{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Wrangling Report</center></h1> \n",
    "    \n",
    "> I was given three sources of data that all needed to be gathered in a different way. The first was a csv file (twitter-archive-enhanced.csv), I downloaded this file from the Udacity website, uploaded it to my Jupyter notebook and  then read it into my jupyter notebook using read_csv and naming the dataset “archives “. The second source was a TSV file (image_predictions.tsv), that was given to me on Udacity’s website in the form of a URL. I downloaded this file programmatically using the requests library and the URL provided, I then read this file in as img_predict creating the second data frame.  The third source we queried the Twitter API to read in each tweets JSON data using the Tweepy library then reading it in line by line creating the final data frame called tweet_df. \n",
    "\n",
    "> Now that we have our three datasets the next step was to assess the data we have. I went through the same process with all 3 datasets, first I did a visual assessment of the data then did some programmatic assessment, assessing one dataframe at a time.  As I assessed the data I made some notes of things that I needed to clean at a later time, documenting these notes as bullets in markdown fields.  In the archives dataframe is where I noticed some issues like the source column having html remnants that needed to be cleaned, multiple columns using erroneous datatypes, erroneous columns, missing data and many null values that were incorrectly labeled as “None”. There were also some tidiness issues that needed to be cleaned up as well. In the img_predict dataframe there were issues that needed to be addressed like column names not being descriptive enough, erroneous datatypes and inconsistencies with uppercase and lowercase values in the same column. In the third and final tweet_df dataframe I noticed that the id column should be changed to string datatype and id should be changed to tweet_id which would allow us to join all three of the tables together as I noticed that all three of these dataframes all had tweet id columns. From a tidiness standpoint I noted this and documented that all 3 datasets should be joined together to make one dataframe that would make cleaning, analyzing and additional assessing much easier and more straight forward. \n",
    "\n",
    "> Next step in the process was to clean the data based off the assessments that I made and documented. First, I made copies of all 3 datasets, changed the id to tweet_id and joined all three dataframes into one dataframe called df_clean. I did some more assessing now that it was all in one dataframe and then began the cleaning process on the new dataframe.  My process was to take one issue on at a time defining the cleaning requirement, coding the changes to clean up the data and immediately checking my work by testing the result. I made many changes to the dataframe such as; changing erroneous datatypes, removing erroneous columns and names, creating new columns to tidy up the dataframe, converting None values to null values and changing column names to be more clear to few. Once I was satisfied with the cleaning performed and fixed the issues that I flagged throughout the assessment process I saved the gathered, assessed and now cleaned df_clean to a CSV file named “twitter-archive-master.csv”.  Now that the gathering, assessing and cleaning was completed the dataframe is ready to be analyzed and visualized. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
